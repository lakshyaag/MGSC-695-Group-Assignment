\graphicspath{{images/}}

\section{Challenges, Learnings \& Future Scope}

\subsection{Challenges} 

Developing custom tree-based algorithms presented two distinct hurdles compared to models covered in this course. 

Firstly, tree-based models involve a different optimization problem at their core, seeking optimal splits at each node based on purity or information gain. 
This contrasts with class-covered models, which optimize coefficients once to minimize a predefined loss function or maximize likelihood using techniques like gradient descent. 
Understanding concepts such as entropy, information gain, bagging, boosting, and ensemble learning required extensive self-study and research. 

Secondly, translating theoretical knowledge into functional code demanded overcoming implementation complexities, including designing efficient data structures, handling categorical variables, and optimizing computational performance. 
Tuning hyperparameters and optimizing model performance without predefined guidelines added further challenges, necessitating iterative experimentation and meticulous testing. 
Despite these hurdles, proactive learning, collaborative problem-solving, and iterative refinement enabled us to overcome challenges and achieve successful outcomes.

\subsection{Key Learnings} 

Key learnings from the project include understanding the optimization challenges inherent in decision tree construction, 
recognizing the trade-offs between tree-based methods such as Random Forests and Gradient Boosting, 
and appreciating the importance of hyperparameters in fine-tuning model performance and avoiding overfitting.

\subsection{Future Scope} 
Looking ahead, we aim to augment our optimization problem by incorporating advanced techniques such as model stacking. 
Additionally, we plan to explore the mathematics of state-of-the-art tree-based models such as LightGBM and XGBoost. 
These advancements will provide opportunities to navigate complex data landscapes with greater ease and accuracy.
